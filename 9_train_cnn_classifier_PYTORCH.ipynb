{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "########### LOAD PKGS ##############\n",
    "####################################\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "\n",
    "#%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "########### MAKE DATA LOADER FUNCTION ###########\n",
    "#################################################\n",
    "\n",
    "# FUNCTION REQUIRED FOR TORCH (?) ALSO FOR RESNET fixed sizes\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.46, 0.48, 0.51], [0.32, 0.32, 0.32])\n",
    "    ])\n",
    "\n",
    "# DATA LOADER AND RANDOMIZER FUNCTION\n",
    "def make_trainloader(train_data, \n",
    "                     vals, \n",
    "                     batch_size,\n",
    "                     randomize=True):\n",
    "    \n",
    "    # RANDOMIZE DATA EVERYTIME THIS IS CALLED\n",
    "    if randomize:\n",
    "        idx = np.random.choice(np.arange(vals.shape[0]),\n",
    "                         vals.shape[0],replace=False)\n",
    "        # REARANGE DATA BASED ON RANDOMIZATION FLAG\n",
    "        train_data = train_data[idx]\n",
    "        vals = vals[idx]\n",
    "    else:\n",
    "        idx = np.arange(vals.shape[0])\n",
    "    \n",
    "\n",
    "    # Compute number of batches\n",
    "    n_batches = train_data.shape[0]//batch_size\n",
    "\n",
    "    # make trainign data plus labels\n",
    "    data_train = []\n",
    "    vals_train = []\n",
    "    for k in range(0,n_batches*batch_size,batch_size):\n",
    "        data_train.append(train_data[k:k+batch_size])\n",
    "        vals_train.append(vals[k:k+batch_size])\n",
    "\n",
    "    # \n",
    "    print (\"# batches: \", n_batches)\n",
    "        \n",
    "    # RATIO OF DATA SPLIT BETWEEN TRAIN AND TEST\n",
    "    split = 0.8\n",
    "    \n",
    "    trainloader = zip(data_train[:int(len(data_train)*split)],\n",
    "                      vals_train[:int(len(data_train)*split)])\n",
    "    \n",
    "    testloader = zip(data_train[int(len(data_train)*split):],\n",
    "                      vals_train[int(len(data_train)*split):])\n",
    "\n",
    "    return trainloader, testloader, n_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function to load images and format for ResNet (n_images, rgb, width, height)\n",
    "def load_data(root_dir, max_count=1E10):\n",
    "    \n",
    "    # TODO: remove RGB EVENTUALLY; Find RESNET50 GREY\n",
    "    # make array to load data from 4 classes\n",
    "    data_loaded = np.zeros((0,3,200,200),'uint8')\n",
    "    vals = []\n",
    "    \n",
    "    # LOAD MONOCROME DATA, USUALLY GREEN CHAN\n",
    "    if False:\n",
    "        for k in range(4):\n",
    "            temp = np.repeat(np.load(root_dir+'/'+str(k)+'.npy')[None],3,axis=0).transpose(1,0,2,3)\n",
    "            data_loaded = np.vstack((data_loaded,temp))\n",
    "            vals.extend(np.zeros(temp.shape[0],'int32')+k)\n",
    "    \n",
    "    # LOAD RGB DATA (but NOTE THAT SECONDARY CHANS ARE messy)\n",
    "    if False:\n",
    "        for k in range(4):\n",
    "            temp = np.load(root_dir+'/'+str(k)+'.npy').transpose(0,3,1,2)\n",
    "            print (temp.shape)\n",
    "            data_loaded = np.vstack((data_loaded,temp))\n",
    "            vals.extend(np.zeros(temp.shape[0],'int32')+k)\n",
    "\n",
    "    # LOAD RGB DATA, COPY GREEN CHAN TO EVERYTHING ELSE\n",
    "    if True:\n",
    "        green_chan = 1\n",
    "        max_trials = max_count\n",
    "        for k in range(4):\n",
    "            temp = np.load(root_dir+'/'+str(k)+'.npy').transpose(0,3,1,2)[:,1]\n",
    "            temp = np.repeat(temp[:,None],3,axis=1)\n",
    "            \n",
    "            if (temp[0,0]-temp[0,1]).sum()!=0:\n",
    "                print (\"BREAK ERROR\")\n",
    "                break\n",
    "                \n",
    "            idx = np.random.choice(np.arange(temp.shape[0]),\n",
    "                                   max_trials,replace=False)\n",
    "            \n",
    "            temp = temp[idx]\n",
    "            print (temp.shape)\n",
    "            data_loaded = np.vstack((data_loaded,temp))\n",
    "            vals.extend(np.zeros(temp.shape[0],'int32')+k)\n",
    "\n",
    "    # convert lables to torch tensors\n",
    "    vals = torch.tensor(vals, dtype=torch.long)\n",
    "\n",
    "    # TRANSFORM DATA AS REQUIRED BY RESNET (?)\n",
    "    train_data = []\n",
    "    from tqdm import trange\n",
    "    for k in trange(vals.shape[0]):\n",
    "        temp2 = train_transforms(data_loaded[k].transpose(1,2,0))\n",
    "        train_data.append(temp2)  #THIS CAN BE DONE FASTER\n",
    "\n",
    "    all_data = torch.stack(train_data)\n",
    "    print (\"Train data final [# samples, RGB, width, height]: \", all_data.shape)\n",
    "\n",
    "    return all_data, vals\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# same as above but for single images\n",
    "def load_data_single_directory(root_dir, save_formated_data=True):\n",
    "    \n",
    "    \n",
    "    max_count = 1E10\n",
    "    \n",
    "    import glob\n",
    "    \n",
    "    # TODO: remove RGB EVENTUALLY; Find RESNET50 GREY\n",
    "    # make array to load data from 4 classes\n",
    "    \n",
    "    fname_save = os.path.join(root_dir,\"data_formated.npz\")\n",
    "    \n",
    "    if os.path.exists(fname_save)==False:\n",
    "\n",
    "        fnames = np.sort(glob.glob(root_dir + '/*.npz'))\n",
    "\n",
    "        # LOAD RGB DATA, COPY GREEN CHAN TO EVERYTHING ELSE\n",
    "        green_chan = 1\n",
    "        max_trials = max_count\n",
    "        data_loaded = [] #np.zeros((0,3,200,200),'uint8')\n",
    "        vals = []\n",
    "        for fname in fnames:\n",
    "            temp = np.load(fname)['frame']\n",
    "            temp = np.repeat(temp[:,:,None],3,axis=2)\n",
    "            if temp.shape[0]!=200:\n",
    "                print ('wrong size: ', temp.shape)\n",
    "            \n",
    "            data_loaded.append(temp)\n",
    "\n",
    "        # make stack of images\n",
    "        data_loaded=np.array(data_loaded)\n",
    "        #print (\"data loaded: \", data_loaded.shape)\n",
    "        # shuffle data; not sure this is needed;\n",
    "        idx = np.random.choice(np.arange(data_loaded.shape[0]),\n",
    "                               data_loaded.shape[0],replace=False)\n",
    "\n",
    "        data_loaded = data_loaded[idx]\n",
    "\n",
    "        vals = np.zeros(data_loaded.shape[0],'int32')\n",
    "\n",
    "        # convert lables to torch tensors\n",
    "\n",
    "        # TRANSFORM DATA AS REQUIRED BY RESNET (?)\n",
    "        train_data = []\n",
    "        from tqdm import trange\n",
    "        for k in trange(vals.shape[0]):\n",
    "            #temp2 = train_transforms(data_loaded[k].transpose(1,2,0))\n",
    "            temp2 = train_transforms(data_loaded[k])\n",
    "            train_data.append(temp2)  #THIS CAN BE DONE FASTER\n",
    "\n",
    "        all_data = torch.stack(train_data)\n",
    "        vals = torch.tensor(vals, dtype=torch.long)\n",
    "        \n",
    "        #all_data = np.array(train_data)\n",
    "        #print (\"Train data final [# samples, RGB, width, height]: \", all_data.shape)\n",
    "        \n",
    "        np.savez(fname_save,\n",
    "                all_data = all_data,\n",
    "                vals=vals)\n",
    "        \n",
    "    else:\n",
    "        data = np.load(fname_save)\n",
    "        all_data = torch.from_numpy(data['all_data'])\n",
    "        vals = torch.from_numpy(data['vals'])\n",
    "    \n",
    "    #all_data = all_data)\n",
    "    #vals = torch.tensor(vals, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    return all_data, vals\n",
    "\n",
    "# DATA LOADER AND RANDOMIZER FUNCTION\n",
    "def make_testloader(train_data, \n",
    "                    batch_size,\n",
    "                    randomize=False):\n",
    "    \n",
    "    # RANDOMIZE DATA EVERYTIME THIS IS CALLED\n",
    "    if randomize:\n",
    "        idx = np.random.choice(np.arange(vals.shape[0]),\n",
    "                         vals.shape[0],replace=False)\n",
    "        # REARANGE DATA BASED ON RANDOMIZATION FLAG\n",
    "        train_data = train_data[idx]\n",
    "\n",
    "    # Compute number of batches\n",
    "    n_batches = train_data.shape[0]//batch_size\n",
    "    if (train_data.shape[0]/batch_size)!= train_data.shape[0]//batch_size:\n",
    "        n_batches+=1\n",
    "\n",
    "    # make test data\n",
    "    data_predict = []\n",
    "    for k in range(0,n_batches*batch_size,batch_size):\n",
    "        data_predict.append(train_data[k:k+batch_size])\n",
    "\n",
    "    # \n",
    "                      \n",
    "    return data_predict, n_batches\n",
    "\n",
    "\n",
    "\n",
    "def plot_bars(predictions, \n",
    "              confidence,\n",
    "              test_data):\n",
    "    \n",
    "    clrs = ['red','blue','cyan','green']\n",
    "    names = ['female','male','pup1 (shaved)','pup2 (unshaved)']\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "    import matplotlib.gridspec as gridspec\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    G = gridspec.GridSpec(4, 4)\n",
    "\n",
    "    # PLOT BAR GRAPHS FOR ALL PREDICTIONS\n",
    "    axes_1 = plt.subplot(G[:1, 0])\n",
    "    plt.title(\"All predicted labels\")\n",
    "    bins = np.arange(-0.5, 4.5, 1)\n",
    "    y = np.histogram(predictions, bins = bins)\n",
    "    for k in range(4):\n",
    "        plt.bar(y[1][k], y[0][k], 0.9, color=clrs[k])\n",
    "    \n",
    "    # add legend\n",
    "    handles, labels = axes_1.get_legend_handles_labels()\n",
    "    for k in range(4):\n",
    "        patch = mpatches.Patch(color=clrs[k], label=names[k])\n",
    "        handles.append(patch) \n",
    "    plt.legend(handles=handles, loc='upper center')\n",
    "    \n",
    "    \n",
    "    # PLOT BAR GRAPHS - THRESHOLD ON CONFIDENCe\n",
    "    axes_1 = plt.subplot(G[1:2, 0])\n",
    "    plt.title(\"Only high confidence labels\")\n",
    "    bins = np.arange(-0.5, 4.5, 1)\n",
    "    \n",
    "    threshold = 0.9\n",
    "    idx_high_conf = np.where(confidence>threshold)[0]\n",
    "    predictions_high_confidence = predictions[idx_high_conf]\n",
    "    \n",
    "    y_high_conf = np.histogram(predictions_high_confidence, bins = bins)\n",
    "    for k in range(4):\n",
    "        plt.bar(y_high_conf[1][k], y_high_conf[0][k], 0.9, color=clrs[k])\n",
    "    \n",
    "    # add legend\n",
    "    handles, labels = axes_1.get_legend_handles_labels()\n",
    "    for k in range(4):\n",
    "        patch = mpatches.Patch(color=clrs[k], label=names[k])\n",
    "        handles.append(patch) \n",
    "    plt.legend(handles=handles, loc='upper center')\n",
    "    \n",
    "    \n",
    "    # MAKE IMAGE PLOTS\n",
    "    max_id = np.argmax(y[0])\n",
    "    print (\"Main animal \", names[max_id])\n",
    "    \n",
    "    examples =[]\n",
    "    example_ids = []\n",
    "    for p in range(4):\n",
    "        if p==max_id:\n",
    "            continue\n",
    "        example_ids.append(p)\n",
    "        idx = np.where(predictions==p)[0]\n",
    "        try:\n",
    "            if idx.shape[0]>=3:\n",
    "                frames = np.random.choice(idx, 3, replace=False)\n",
    "            else:\n",
    "                frames = np.random.choice(idx, 3)\n",
    "        except:\n",
    "            frames = [0,0,0]\n",
    "            \n",
    "        examples.append(frames)\n",
    "    \n",
    "    for k in range(3):\n",
    "        ctr = 0\n",
    "        frames = examples[k]\n",
    "        for p in range(3):\n",
    "            ax = plt.subplot(G[k,p+1])\n",
    "\n",
    "            # get image\n",
    "            temp = test_data[frames[ctr]].cpu().detach().numpy().transpose(1,2,0)\n",
    "            plt.imshow(temp)\n",
    "\n",
    "            plt.title(\"fr: \"+str(frames[ctr])+ \", \"+\n",
    "                     names[predictions[frames[ctr]]])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            ctr+=1\n",
    "            \n",
    "            if p==0:\n",
    "                plt.ylabel(\"examples \\n\"+str(names[example_ids[k]]))\n",
    "\n",
    "\n",
    "    # PLOT TIME\n",
    "    axes_2 = plt.subplot(G[3, :])\n",
    "    clr_out = []\n",
    "    for k in range(predictions.shape[0]):\n",
    "        clr_out.append(clrs[predictions[k]])\n",
    "\n",
    "    time = np.arange(predictions.shape[0])/25.\n",
    "    plt.scatter(time, \n",
    "             np.ones(predictions.shape[0]),\n",
    "             c=clr_out)\n",
    "    \n",
    "    # \n",
    "    clr_out = []\n",
    "    for k in range(predictions_high_confidence.shape[0]):\n",
    "        clr_out.append(clrs[predictions_high_confidence[k]])\n",
    "        \n",
    "    time_high_conf = idx_high_conf/25.\n",
    "    plt.scatter(time_high_conf, \n",
    "             np.ones(predictions_high_confidence.shape[0])+1,\n",
    "             c=clr_out)\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Time (sec)\", fontsize=20)\n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.yticks([])\n",
    "    plt.suptitle(\"CNN animal detected: \"+names[max_id] + \"(all frames) \"\n",
    "                 + str(round(np.max(y[0])/np.sum(y[0])*100,2))+\"% of total track\"\n",
    "                 \n",
    "                 + \"\\nCNN animal detected (high confidence predictoin only): \"+names[max_id] + \" \"\n",
    "                 + str(round(np.max(y_high_conf[0])/np.sum(y_high_conf[0])*100,2))+\"% of total track\"\n",
    "                 + \"\\n SLEAP tracklet # \" + selected_track \n",
    "                 + \" (# frames in track \" \n",
    "                 +str(predictions.shape[0])+\")\", fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (4): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "########### LOAD RESNET ############\n",
    "####################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Not sure what this does\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Note sure what this does, effect on fc layer?\n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(512, 4),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "\n",
    "# todo: look up this loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# todo: look up this optimizer\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# move model to gpu\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([4, 512]).\n\tsize mismatch for fc.2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d6377a1e3bfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/model_100epoch_Green3Chan_cnn_training_30mins_data.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([4, 512]).\n\tsize mismatch for fc.2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([4])."
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "##### LOAD PREVIOUSLY SAVED MODEL (OPTIONAL) ######\n",
    "###################################################\n",
    "fname = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/model_100epoch_Green3Chan_cnn_training_30mins_data.pt'\n",
    "model.load_state_dict(torch.load(fname))\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2300, 3, 200, 200)\n",
      "(2300, 3, 200, 200)\n",
      "(2300, 3, 200, 200)\n",
      "(2300, 3, 200, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9200/9200 [00:23<00:00, 396.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data final [# samples, RGB, width, height]:  torch.Size([9200, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "########### LOAD DATA ##############\n",
    "####################################\n",
    "\n",
    "max_count = 2300\n",
    "data_location = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_training_30mins/animals'\n",
    "all_data, vals = load_data(data_location, max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  0\n",
      "# batches:  18\n",
      "labels:  tensor([1, 0, 2, 2, 0, 1, 0, 0, 2, 3], device='cuda:0')\n",
      "predictions:  tensor([1, 1, 1, 2, 1, 1, 1, 1, 1, 3], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.9227 Acc: 0.5540\n",
      "epochs:  1\n",
      "# batches:  18\n",
      "labels:  tensor([0, 2, 1, 0, 3, 2, 1, 2, 2, 2], device='cuda:0')\n",
      "predictions:  tensor([0, 2, 1, 0, 0, 2, 1, 2, 2, 2], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.5692 Acc: 0.8100\n",
      "epochs:  2\n",
      "# batches:  18\n",
      "labels:  tensor([2, 2, 1, 0, 2, 0, 2, 0, 0, 2], device='cuda:0')\n",
      "predictions:  tensor([2, 2, 1, 0, 1, 0, 3, 0, 0, 2], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.4046 Acc: 0.8700\n",
      "epochs:  3\n",
      "# batches:  18\n",
      "labels:  tensor([1, 3, 3, 0, 1, 3, 3, 2, 0, 0], device='cuda:0')\n",
      "predictions:  tensor([1, 2, 3, 2, 1, 3, 3, 2, 0, 0], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.3129 Acc: 0.8900\n",
      "epochs:  4\n",
      "# batches:  18\n",
      "labels:  tensor([2, 0, 1, 2, 3, 2, 3, 0, 2, 1], device='cuda:0')\n",
      "predictions:  tensor([2, 0, 1, 2, 1, 2, 1, 0, 2, 1], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.2606 Acc: 0.9200\n",
      "epochs:  5\n",
      "# batches:  18\n",
      "labels:  tensor([2, 1, 3, 3, 2, 1, 0, 0, 2, 2], device='cuda:0')\n",
      "predictions:  tensor([2, 1, 3, 3, 2, 1, 0, 0, 2, 2], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.2237 Acc: 0.9220\n",
      "epochs:  6\n",
      "# batches:  18\n",
      "labels:  tensor([2, 2, 0, 1, 2, 2, 1, 0, 0, 1], device='cuda:0')\n",
      "predictions:  tensor([2, 2, 0, 1, 2, 2, 1, 0, 0, 1], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1913 Acc: 0.9440\n",
      "epochs:  7\n",
      "# batches:  18\n",
      "labels:  tensor([1, 1, 0, 0, 1, 2, 3, 3, 1, 1], device='cuda:0')\n",
      "predictions:  tensor([1, 1, 0, 0, 1, 2, 3, 3, 1, 1], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1477 Acc: 0.9520\n",
      "epochs:  8\n",
      "# batches:  18\n",
      "labels:  tensor([2, 1, 2, 0, 0, 0, 3, 1, 3, 3], device='cuda:0')\n",
      "predictions:  tensor([2, 1, 2, 0, 0, 0, 3, 1, 3, 3], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1561 Acc: 0.9480\n",
      "epochs:  9\n",
      "# batches:  18\n",
      "labels:  tensor([0, 2, 2, 0, 1, 1, 2, 0, 3, 2], device='cuda:0')\n",
      "predictions:  tensor([0, 2, 2, 0, 1, 1, 2, 0, 3, 2], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1256 Acc: 0.9600\n",
      "epochs:  10\n",
      "# batches:  18\n",
      "labels:  tensor([1, 2, 3, 1, 1, 2, 2, 3, 3, 0], device='cuda:0')\n",
      "predictions:  tensor([1, 2, 2, 1, 1, 2, 2, 3, 3, 0], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1473 Acc: 0.9620\n",
      "epochs:  11\n",
      "# batches:  18\n",
      "labels:  tensor([0, 3, 3, 3, 3, 3, 1, 3, 3, 0], device='cuda:0')\n",
      "predictions:  tensor([0, 3, 3, 3, 3, 3, 1, 3, 3, 0], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1015 Acc: 0.9700\n",
      "epochs:  12\n",
      "# batches:  18\n",
      "labels:  tensor([2, 0, 1, 1, 3, 0, 3, 1, 1, 3], device='cuda:0')\n",
      "predictions:  tensor([2, 0, 1, 1, 3, 0, 0, 1, 1, 3], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1060 Acc: 0.9660\n",
      "epochs:  13\n",
      "# batches:  18\n",
      "labels:  tensor([3, 0, 2, 0, 1, 3, 1, 1, 0, 3], device='cuda:0')\n",
      "predictions:  tensor([3, 0, 2, 0, 1, 3, 1, 1, 0, 3], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.0915 Acc: 0.9760\n",
      "epochs:  14\n",
      "# batches:  18\n",
      "labels:  tensor([2, 0, 1, 3, 1, 0, 1, 2, 1, 3], device='cuda:0')\n",
      "predictions:  tensor([2, 0, 1, 3, 1, 0, 1, 2, 1, 2], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1506 Acc: 0.9480\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "############## TRAIN MODEL ## ###################\n",
    "#################################################\n",
    "\n",
    "epochs = 15\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    print (\"epochs: \", epoch)\n",
    "    \n",
    "    trainloader, testloader, n_batches = make_trainloader(all_data, \n",
    "                                                          vals, \n",
    "                                                          batch_size=500)\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    n_trials=0\n",
    "    ctr=0\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        n_trials+= labels.shape[0]\n",
    "        #print (inputs.shape)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        last_inputs=torch.clone(inputs)\n",
    "        last_labels=torch.clone(labels)\n",
    "\n",
    "        \n",
    "        # ZERO INit\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # PREDICT;\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "                \n",
    "#         if False:# ctr%10==0:\n",
    "#             print (labels[:10])\n",
    "#             print (preds[:10])\n",
    "#             print ('')\n",
    "\n",
    "        # backward + optimize only if in training phase\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track performance \n",
    "        if False:\n",
    "            # ON TRAIN DATA\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "   # else:\n",
    "    if True:\n",
    "        n_trials=0\n",
    "        # test only on first train dataset\n",
    "        for inputs, labels in testloader:\n",
    "            \n",
    "            n_trials+= labels.shape[0]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if True:# ctr%10==0:\n",
    "                print (\"labels: \", labels[:10])\n",
    "                print (\"predictions: \", preds[:10])\n",
    "                print ('')\n",
    "\n",
    "        \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds.data == labels.data)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    epoch_loss = running_loss / n_trials\n",
    "    epoch_acc = running_corrects / n_trials\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        ctr, epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############## SAVE MODEL #####################\n",
    "###############################################\n",
    "root_dir = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/'\n",
    "model_name = 'model.pt'\n",
    "\n",
    "torch.save(model.state_dict(), root_dir+model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "######### PREPROCESS TEST DATA ##########\n",
    "#########################################\n",
    "root_dirs = ['/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/0',\n",
    "'/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/1',\n",
    "'/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/2']\n",
    "\n",
    "\n",
    "for root_dir in root_dirs:\n",
    "    load_data_single_directory(root_dir)\n",
    "\n",
    "print (\"Done\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "######### PREDICT ON TEST DATA ##########\n",
    "#########################################\n",
    "\n",
    "root_dirs = ['/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/0',\n",
    "'/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/1',\n",
    "'/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/2']\n",
    "\n",
    "\n",
    "for root_dir in root_dirs:\n",
    "    \n",
    "    predict...(root_dir)\n",
    "\n",
    "print (\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing track: 446\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "######### PREDICT ON TEST DATA ##########\n",
    "#########################################\n",
    "\n",
    "def load_training_data_run_prediction(fname_track, \n",
    "                                      model,\n",
    "                                      device,\n",
    "                                      recompute=False):\n",
    "    \n",
    "    max_count=1E10\n",
    "    \n",
    "    fname_out = os.path.join(fname_track,\"predictions.npz\")\n",
    "    \n",
    "    if os.path.exists(fname_out)==False:\n",
    "\n",
    "        test_data, test_vals = load_data_single_directory(fname_track)\n",
    "        #print (\"test data size: \", test_data.shape)\n",
    "\n",
    "\n",
    "        # change model to evaluation mode to avoid batch normalization\n",
    "        model.eval()\n",
    "\n",
    "        # load the test data\n",
    "        test_loader, n_batches = make_testloader(test_data, \n",
    "                                                  batch_size=500)\n",
    "\n",
    "        #print (\" # batches: \", len(test_loader), \"  shape : \", test_loader[0].shape)\n",
    "\n",
    "        predictions = []\n",
    "        output_array = []\n",
    "        for inputs in test_loader:\n",
    "            # load to device\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            n_trials = inputs.shape[0]\n",
    "\n",
    "            # PREDICT;\n",
    "            outputs = model(inputs)\n",
    "            output_array.extend(outputs.cpu().detach().numpy())\n",
    "\n",
    "            # get best predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        #print (\"predictions: \", predictions.shape, predictions[:10])\n",
    "\n",
    "        #probs = predictions \n",
    "        output_array = np.array(output_array)\n",
    "        #print (\"output array: \", output_array.shape)\n",
    "        sig_pred = 1 / (np.exp(-output_array))  # confidence map\n",
    "\n",
    "        confidence = []\n",
    "        for k in range(sig_pred.shape[0]):\n",
    "            confidence.append(sig_pred[k][predictions[k]])\n",
    "        confidence=np.array(confidence)\n",
    "        #print (\"confidence; \", confidence[:10])\n",
    "        \n",
    "        np.savez(fname_out,\n",
    "                 predictions=predictions,\n",
    "                 confidence=confidence)\n",
    "        \n",
    "    else:\n",
    "        data = np.load(fname_out)\n",
    "        predictions = data['predictions']\n",
    "        confidence = data['confidence']\n",
    "        test_data = None\n",
    "    \n",
    "    return predictions, test_data, confidence\n",
    "\n",
    "\n",
    "root_dir = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/march_9/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/'\n",
    "selected_track = '446'\n",
    "fname = root_dir+selected_track\n",
    "print (\"processing track:\", selected_track)\n",
    "\n",
    "predictions, test_data, confidence = load_training_data_run_prediction(fname, \n",
    "                                                             model,\n",
    "                                                             device)\n",
    "\n",
    "\n",
    "# TO REFORMAT DATA TO .npz file as before\n",
    "\n",
    "test = \n",
    "\n",
    "#(ctr, ids, classes, logits) = predict_cnn2(x_test, y_test)\n",
    "  \n",
    "classes = np.hstack(classes)\n",
    "print (\"classes: \", classes)\n",
    "\n",
    "np.savez(root_dir+selected_output+'/classification_output.npz',\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        vals=vals,\n",
    "        classes=classes,\n",
    "        frames_array=frames_array)\n",
    "\n",
    "\n",
    "print ('done')\n",
    "# REVIEW OUTPUT\n",
    "\n",
    "# thresh = 0.8\n",
    "# idx = np.where(confidence>thresh)[0]\n",
    "# a, b = np.unique(predictions[idx], return_counts=True)\n",
    "# if a.shape[0]>0:\n",
    "#     print (\"ids: \", a, \" counts: \", b)\n",
    "#     animal_best = a[np.argmax(b)]\n",
    "#     highest = np.max(b)/predictions[idx].shape[0]\n",
    "#     print ('Best animal predicted: ', animal_best, \"  purity: \", highest, \n",
    "#            \" # total frames: \", predictions.shape[0],\n",
    "#           \" # frames > threshold: \", idx.shape[0])\n",
    "#     #print (predictions[:10])\n",
    "# else:\n",
    "#     print(\"no images over threhsold\")\n",
    "\n",
    "# for k in range(4):\n",
    "#     ax=plt.subplot(2,2,k+1)\n",
    "#     try:\n",
    "#         idx = np.random.choice(np.arange(test_data.shape[0]))\n",
    "#         plt.imshow(test_data[idx].squeeze().cpu().detach().numpy().transpose(1,2,0))\n",
    "#     except:\n",
    "#         pass\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main animal  female\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########### PLOT RESULTS ###############\n",
    "########################################\n",
    "    \n",
    "plot_bars(predictions, confidence, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
