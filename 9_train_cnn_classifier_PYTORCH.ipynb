{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "########### LOAD PKGS ##############\n",
    "####################################\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "\n",
    "#%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (4): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "########### LOAD RESNET ############\n",
    "####################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Not sure what this does\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Note sure what this does, effect on fc layer?\n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(512, 4),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "\n",
    "# todo: look up this loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# todo: look up this optimizer\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# move model to gpu\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "##### LOAD PREVIOUSLY SAVED MODEL (OPTIONAL) ######\n",
    "###################################################\n",
    "fname = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/model_100epoch_Green3Chan_cnn_training_30mins_data.pt'\n",
    "model.load_state_dict(torch.load(fname))\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "########### MAKE DATA LOADER FUNCTION ###########\n",
    "#################################################\n",
    "\n",
    "# FUNCTION REQUIRED FOR TORCH (?) ALSO FOR RESNET fixed sizes\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.46, 0.48, 0.51], [0.32, 0.32, 0.32])\n",
    "    ])\n",
    "\n",
    "# DATA LOADER AND RANDOMIZER FUNCTION\n",
    "def make_trainloader(train_data, \n",
    "                     vals, \n",
    "                     batch_size,\n",
    "                     randomize=True):\n",
    "    \n",
    "    # RANDOMIZE DATA EVERYTIME THIS IS CALLED\n",
    "    if randomize:\n",
    "        idx = np.random.choice(np.arange(vals.shape[0]),\n",
    "                         vals.shape[0],replace=False)\n",
    "        # REARANGE DATA BASED ON RANDOMIZATION FLAG\n",
    "        train_data = train_data[idx]\n",
    "        vals = vals[idx]\n",
    "    else:\n",
    "        idx = np.arange(vals.shape[0])\n",
    "    \n",
    "\n",
    "    # Compute number of batches\n",
    "    n_batches = train_data.shape[0]//batch_size\n",
    "\n",
    "    # make trainign data plus labels\n",
    "    data_train = []\n",
    "    vals_train = []\n",
    "    for k in range(0,n_batches*batch_size,batch_size):\n",
    "        data_train.append(train_data[k:k+batch_size])\n",
    "        vals_train.append(vals[k:k+batch_size])\n",
    "\n",
    "    # \n",
    "    print (\"# batches: \", n_batches)\n",
    "        \n",
    "    # RATIO OF DATA SPLIT BETWEEN TRAIN AND TEST\n",
    "    split = 0.8\n",
    "    \n",
    "    trainloader = zip(data_train[:int(len(data_train)*split)],\n",
    "                      vals_train[:int(len(data_train)*split)])\n",
    "    \n",
    "    testloader = zip(data_train[int(len(data_train)*split):],\n",
    "                      vals_train[int(len(data_train)*split):])\n",
    "\n",
    "    return trainloader, testloader, n_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function to load images and format for ResNet (n_images, rgb, width, height)\n",
    "def load_data(root_dir, max_count=1E10):\n",
    "    \n",
    "    # TODO: remove RGB EVENTUALLY; Find RESNET50 GREY\n",
    "    # make array to load data from 4 classes\n",
    "    data_loaded = np.zeros((0,3,200,200),'uint8')\n",
    "    vals = []\n",
    "    \n",
    "    # LOAD MONOCROME DATA, USUALLY GREEN CHAN\n",
    "    if False:\n",
    "        for k in range(4):\n",
    "            temp = np.repeat(np.load(root_dir+'/'+str(k)+'.npy')[None],3,axis=0).transpose(1,0,2,3)\n",
    "            data_loaded = np.vstack((data_loaded,temp))\n",
    "            vals.extend(np.zeros(temp.shape[0],'int32')+k)\n",
    "    \n",
    "    # LOAD RGB DATA (but NOTE THAT SECONDARY CHANS ARE messy)\n",
    "    if False:\n",
    "        for k in range(4):\n",
    "            temp = np.load(root_dir+'/'+str(k)+'.npy').transpose(0,3,1,2)\n",
    "            print (temp.shape)\n",
    "            data_loaded = np.vstack((data_loaded,temp))\n",
    "            vals.extend(np.zeros(temp.shape[0],'int32')+k)\n",
    "\n",
    "    # LOAD RGB DATA, COPY GREEN CHAN TO EVERYTHING ELSE\n",
    "    if True:\n",
    "        green_chan = 1\n",
    "        max_trials = max_count\n",
    "        for k in range(4):\n",
    "            temp = np.load(root_dir+'/'+str(k)+'.npy').transpose(0,3,1,2)[:,1]\n",
    "            temp = np.repeat(temp[:,None],3,axis=1)\n",
    "            \n",
    "            if (temp[0,0]-temp[0,1]).sum()!=0:\n",
    "                print (\"BREAK ERROR\")\n",
    "                break\n",
    "                \n",
    "            idx = np.random.choice(np.arange(temp.shape[0]),\n",
    "                                   max_trials,replace=False)\n",
    "            \n",
    "            temp = temp[idx]\n",
    "            print (temp.shape)\n",
    "            data_loaded = np.vstack((data_loaded,temp))\n",
    "            vals.extend(np.zeros(temp.shape[0],'int32')+k)\n",
    "\n",
    "    # convert lables to torch tensors\n",
    "    vals = torch.tensor(vals, dtype=torch.long)\n",
    "\n",
    "    # TRANSFORM DATA AS REQUIRED BY RESNET (?)\n",
    "    train_data = []\n",
    "    from tqdm import trange\n",
    "    for k in trange(vals.shape[0]):\n",
    "        temp2 = train_transforms(data_loaded[k].transpose(1,2,0))\n",
    "        train_data.append(temp2)  #THIS CAN BE DONE FASTER\n",
    "\n",
    "    all_data = torch.stack(train_data)\n",
    "    print (\"Train data final [# samples, RGB, width, height]: \", all_data.shape)\n",
    "\n",
    "    return all_data, vals\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2300, 3, 200, 200)\n",
      "(2300, 3, 200, 200)\n",
      "(2300, 3, 200, 200)\n",
      "(2300, 3, 200, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9200/9200 [00:21<00:00, 419.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data final [# samples, RGB, width, height]:  torch.Size([9200, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "########### LOAD DATA ##############\n",
    "####################################\n",
    "\n",
    "max_count = 2300\n",
    "data_location = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/cnn_training_30mins/animals'\n",
    "all_data, vals = load_data(data_location, max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  0\n",
      "# batches:  18\n",
      "labels:  tensor([3, 1, 0, 2, 0, 2, 2, 2, 2, 0], device='cuda:0')\n",
      "predictions:  tensor([3, 1, 0, 3, 0, 2, 2, 3, 2, 0], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.2503 Acc: 0.9260\n",
      "epochs:  1\n",
      "# batches:  18\n",
      "labels:  tensor([2, 3, 1, 3, 0, 1, 1, 1, 3, 1], device='cuda:0')\n",
      "predictions:  tensor([2, 3, 2, 3, 0, 1, 1, 1, 2, 1], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.2333 Acc: 0.9120\n",
      "epochs:  2\n",
      "# batches:  18\n",
      "labels:  tensor([1, 3, 3, 3, 0, 2, 2, 2, 1, 0], device='cuda:0')\n",
      "predictions:  tensor([1, 2, 3, 3, 0, 2, 2, 2, 1, 0], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1372 Acc: 0.9660\n",
      "epochs:  3\n",
      "# batches:  18\n",
      "labels:  tensor([1, 0, 3, 2, 0, 1, 0, 3, 0, 3], device='cuda:0')\n",
      "predictions:  tensor([1, 0, 3, 2, 0, 1, 0, 3, 0, 3], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1341 Acc: 0.9620\n",
      "epochs:  4\n",
      "# batches:  18\n",
      "labels:  tensor([0, 0, 1, 0, 1, 1, 1, 3, 2, 1], device='cuda:0')\n",
      "predictions:  tensor([0, 0, 1, 0, 1, 1, 1, 3, 2, 1], device='cuda:0')\n",
      "\n",
      "0 Loss: 0.1381 Acc: 0.9540\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "############## TRAIN MODEL ## ###################\n",
    "#################################################\n",
    "\n",
    "epochs = 5\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    print (\"epochs: \", epoch)\n",
    "    \n",
    "    trainloader, testloader, n_batches = make_trainloader(all_data, \n",
    "                                                          vals, \n",
    "                                                          batch_size=500)\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    n_trials=0\n",
    "    ctr=0\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        n_trials+= labels.shape[0]\n",
    "        #print (inputs.shape)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        last_inputs=torch.clone(inputs)\n",
    "        last_labels=torch.clone(labels)\n",
    "\n",
    "        \n",
    "        # ZERO INit\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # PREDICT;\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "                \n",
    "#         if False:# ctr%10==0:\n",
    "#             print (labels[:10])\n",
    "#             print (preds[:10])\n",
    "#             print ('')\n",
    "\n",
    "        # backward + optimize only if in training phase\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track performance \n",
    "        if False:\n",
    "            # ON TRAIN DATA\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "   # else:\n",
    "    if True:\n",
    "        n_trials=0\n",
    "        # test only on first train dataset\n",
    "        for inputs, labels in testloader:\n",
    "            \n",
    "            n_trials+= labels.shape[0]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if True:# ctr%10==0:\n",
    "                print (\"labels: \", labels[:10])\n",
    "                print (\"predictions: \", preds[:10])\n",
    "                print ('')\n",
    "\n",
    "        \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds.data == labels.data)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    epoch_loss = running_loss / n_trials\n",
    "    epoch_acc = running_corrects / n_trials\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        ctr, epoch_loss, epoch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############## SAVE MODEL #####################\n",
    "###############################################\n",
    "root_dir = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/'\n",
    "model_name = 'model_100epoch_Green3Chan_cnn_training_30mins_data.pt'\n",
    "\n",
    "torch.save(model.state_dict(), root_dir+model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final inputs:  torch.Size([500, 3, 224, 224])\n",
      "0 tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "predictions:  (500,) [0 0 0 0 0 0 0 0 0 0]\n",
      "final inputs:  torch.Size([500, 3, 224, 224])\n",
      "500 tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "predictions:  (500,) [1 1 1 1 1 1 1 1 1 1]\n",
      "final inputs:  torch.Size([500, 3, 224, 224])\n",
      "1000 tensor([2, 2, 2, 2, 3, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "predictions:  (500,) [2 2 2 2 3 2 2 2 2 2]\n",
      "final inputs:  torch.Size([500, 3, 224, 224])\n",
      "1500 tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')\n",
      "predictions:  (500,) [3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "############# LOAD TEST DATA ############\n",
    "###############################################\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if False:\n",
    "    data_location = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/cnn_training_30mins/animals/temp'\n",
    "    test_data, test_vals = load_data(data_location,500)\n",
    "    print (test_data.shape, test_vals)\n",
    "\n",
    "for p in range(0,2000,500):\n",
    "    inputs = test_data[p:p+500]\n",
    "   \n",
    "    input_array = []\n",
    "    for k in range(inputs.shape[0]):\n",
    "        temp=inputs[k]#.transpose(0,1,2)\n",
    "        temp = train_transforms(temp)\n",
    "        input_array.append(temp)\n",
    "        \n",
    "    inputs = torch.stack(input_array).to(device)\n",
    "  \n",
    "    print (\"final inputs: \", inputs.shape)\n",
    "\n",
    "    # PREDICT;\n",
    "    # ZERO INit\n",
    "    #optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    #preds_array.extend(preds)\n",
    "    \n",
    "    #preds_array = preds\n",
    "    print (p,preds[:10])\n",
    "    \n",
    "    \n",
    "    predictions= preds.cpu().detach().numpy()\n",
    "    print (\"predictions: \", predictions.shape, predictions[:10])\n",
    "\n",
    "    ########################################\n",
    "    ########### PLOT RESULTS ###############\n",
    "    ########################################\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    G = gridspec.GridSpec(2, 4)\n",
    "\n",
    "    axes_1 = plt.subplot(G[0, 0])\n",
    "    bins = np.arange(-1, 5, 1)\n",
    "    y = np.histogram(predictions, bins = bins)\n",
    "    plt.bar(y[1][:-1], y[0], 0.9)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "\n",
    "    axes_2 = plt.subplot(G[1, :])\n",
    "    plt.scatter(np.arange(predictions.shape[0]), \n",
    "             np.ones(predictions.shape[0]),\n",
    "             c=predictions)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# same as above but for single images\n",
    "def load_data_single_directory(root_dir, max_count=1E10):\n",
    "    \n",
    "    import glob\n",
    "    \n",
    "    # TODO: remove RGB EVENTUALLY; Find RESNET50 GREY\n",
    "    # make array to load data from 4 classes\n",
    "    \n",
    "    fnames = np.sort(glob.glob(root_dir + '/*.npz'))\n",
    "    #print (fnames)\n",
    "\n",
    "    # LOAD RGB DATA, COPY GREEN CHAN TO EVERYTHING ELSE\n",
    "    green_chan = 1\n",
    "    max_trials = max_count\n",
    "    data_loaded = [] #np.zeros((0,3,200,200),'uint8')\n",
    "    vals = []\n",
    "    for fname in fnames:\n",
    "        temp = np.load(fname)['frame']\n",
    "        #print (temp.shape)\n",
    "        #.transpose(0,3,1,2)[:,1]\n",
    "        temp = np.repeat(temp[:,:,None],3,axis=2)\n",
    "        #print (temp.shape)\n",
    "\n",
    "        data_loaded.append(temp)\n",
    "    \n",
    "    # make stack of images\n",
    "    data_loaded=np.array(data_loaded)\n",
    "    print (\"data loaded: \", data_loaded.shape)\n",
    "    # shuffle data; not sure this is needed;\n",
    "    idx = np.random.choice(np.arange(data_loaded.shape[0]),\n",
    "                           data_loaded.shape[0],replace=False)\n",
    "    \n",
    "    data_loaded = data_loaded[idx]\n",
    "\n",
    "    vals = np.zeros(data_loaded.shape[0],'int32')\n",
    "\n",
    "    # convert lables to torch tensors\n",
    "    vals = torch.tensor(vals, dtype=torch.long)\n",
    "\n",
    "    # TRANSFORM DATA AS REQUIRED BY RESNET (?)\n",
    "    train_data = []\n",
    "    from tqdm import trange\n",
    "    for k in trange(vals.shape[0]):\n",
    "        #temp2 = train_transforms(data_loaded[k].transpose(1,2,0))\n",
    "        temp2 = train_transforms(data_loaded[k])\n",
    "        train_data.append(temp2)  #THIS CAN BE DONE FASTER\n",
    "\n",
    "    all_data = torch.stack(train_data)\n",
    "    print (\"Train data final [# samples, RGB, width, height]: \", all_data.shape)\n",
    "\n",
    "    return all_data, vals\n",
    "\n",
    "# DATA LOADER AND RANDOMIZER FUNCTION\n",
    "def make_testloader(train_data, \n",
    "                    batch_size,\n",
    "                    randomize=False):\n",
    "    \n",
    "    # RANDOMIZE DATA EVERYTIME THIS IS CALLED\n",
    "    if randomize:\n",
    "        idx = np.random.choice(np.arange(vals.shape[0]),\n",
    "                         vals.shape[0],replace=False)\n",
    "        # REARANGE DATA BASED ON RANDOMIZATION FLAG\n",
    "        train_data = train_data[idx]\n",
    "\n",
    "    # Compute number of batches\n",
    "    n_batches = train_data.shape[0]//batch_size\n",
    "    if (train_data.shape[0]/batch_size)!= train_data.shape[0]//batch_size:\n",
    "        n_batches+=1\n",
    "\n",
    "    # make test data\n",
    "    data_predict = []\n",
    "    for k in range(0,n_batches*batch_size,batch_size):\n",
    "        data_predict.append(train_data[k:k+batch_size])\n",
    "\n",
    "    # \n",
    "                      \n",
    "    return data_predict, n_batches\n",
    "\n",
    "\n",
    "def load_training_data_master(fname, \n",
    "                             model,\n",
    "                             device):\n",
    "    \n",
    "    test_data, test_vals = load_data_single_directory(fname, max_count=1E10)\n",
    "    print (\"test data size: \", test_data.shape)\n",
    "\n",
    "\n",
    "    # change model to evaluation mode to avoid batch normalization\n",
    "    model.eval()\n",
    "\n",
    "    # load the test data\n",
    "    test_loader, n_batches = make_testloader(test_data, \n",
    "                                              batch_size=500)\n",
    "\n",
    "    print (\" # batches: \", len(test_loader), \"  shape : \", test_loader[0].shape)\n",
    "\n",
    "    predictions = []\n",
    "    output_array = []\n",
    "    for inputs in test_loader:\n",
    "        # load to device\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        n_trials = inputs.shape[0]\n",
    "\n",
    "        # PREDICT;\n",
    "        outputs = model(inputs)\n",
    "        output_array.extend(outputs.cpu().detach().numpy())\n",
    "        \n",
    "        # get best predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    print (\"predictions: \", predictions.shape, predictions[:10])\n",
    "    \n",
    "    #probs = predictions \n",
    "    output_array = np.array(output_array)\n",
    "    print (\"output array: \", output_array.shape)\n",
    "    sig_pred = 1 / (np.exp(-output_array))  # confidence map\n",
    "\n",
    "    confidence = []\n",
    "    for k in range(sig_pred.shape[0]):\n",
    "        confidence.append(sig_pred[k][predictions[k]])\n",
    "    confidence=np.array(confidence)\n",
    "    print (\"confidence; \", confidence[:10])\n",
    "\n",
    "    \n",
    "    return predictions, test_data, confidence\n",
    "\n",
    "def plot_bars(predictions, \n",
    "              confidence,\n",
    "              test_data):\n",
    "    \n",
    "    clrs = ['red','blue','cyan','green']\n",
    "    names = ['female','male','pup1 (shaved)','pup2 (unshaved)']\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "    import matplotlib.gridspec as gridspec\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    G = gridspec.GridSpec(4, 4)\n",
    "\n",
    "    # PLOT BAR GRAPHS FOR ALL PREDICTIONS\n",
    "    axes_1 = plt.subplot(G[:1, 0])\n",
    "    plt.title(\"All predicted labels\")\n",
    "    bins = np.arange(-0.5, 4.5, 1)\n",
    "    y = np.histogram(predictions, bins = bins)\n",
    "    for k in range(4):\n",
    "        plt.bar(y[1][k], y[0][k], 0.9, color=clrs[k])\n",
    "    \n",
    "    # add legend\n",
    "    handles, labels = axes_1.get_legend_handles_labels()\n",
    "    for k in range(4):\n",
    "        patch = mpatches.Patch(color=clrs[k], label=names[k])\n",
    "        handles.append(patch) \n",
    "    plt.legend(handles=handles, loc='upper center')\n",
    "    \n",
    "    \n",
    "    # PLOT BAR GRAPHS - THRESHOLD ON CONFIDENCe\n",
    "    axes_1 = plt.subplot(G[1:2, 0])\n",
    "    plt.title(\"Only high confidence labels\")\n",
    "    bins = np.arange(-0.5, 4.5, 1)\n",
    "    \n",
    "    threshold = 0.9\n",
    "    idx_high_conf = np.where(confidence>threshold)[0]\n",
    "    predictions_high_confidence = predictions[idx_high_conf]\n",
    "    \n",
    "    y_high_conf = np.histogram(predictions_high_confidence, bins = bins)\n",
    "    for k in range(4):\n",
    "        plt.bar(y_high_conf[1][k], y_high_conf[0][k], 0.9, color=clrs[k])\n",
    "    \n",
    "    # add legend\n",
    "    handles, labels = axes_1.get_legend_handles_labels()\n",
    "    for k in range(4):\n",
    "        patch = mpatches.Patch(color=clrs[k], label=names[k])\n",
    "        handles.append(patch) \n",
    "    plt.legend(handles=handles, loc='upper center')\n",
    "    \n",
    "    \n",
    "    # MAKE IMAGE PLOTS\n",
    "    max_id = np.argmax(y[0])\n",
    "    print (\"Main animal \", names[max_id])\n",
    "    \n",
    "    examples =[]\n",
    "    example_ids = []\n",
    "    for p in range(4):\n",
    "        if p==max_id:\n",
    "            continue\n",
    "        example_ids.append(p)\n",
    "        idx = np.where(predictions==p)[0]\n",
    "        try:\n",
    "            if idx.shape[0]>=3:\n",
    "                frames = np.random.choice(idx, 3, replace=False)\n",
    "            else:\n",
    "                frames = np.random.choice(idx, 3)\n",
    "        except:\n",
    "            frames = [0,0,0]\n",
    "            \n",
    "        examples.append(frames)\n",
    "    \n",
    "    for k in range(3):\n",
    "        ctr = 0\n",
    "        frames = examples[k]\n",
    "        for p in range(3):\n",
    "            ax = plt.subplot(G[k,p+1])\n",
    "\n",
    "            # get image\n",
    "            temp = test_data[frames[ctr]].cpu().detach().numpy().transpose(1,2,0)\n",
    "            plt.imshow(temp)\n",
    "\n",
    "            plt.title(\"fr: \"+str(frames[ctr])+ \", \"+\n",
    "                     names[predictions[frames[ctr]]])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            ctr+=1\n",
    "            \n",
    "            if p==0:\n",
    "                plt.ylabel(\"examples \\n\"+str(names[example_ids[k]]))\n",
    "\n",
    "\n",
    "    # PLOT TIME\n",
    "    axes_2 = plt.subplot(G[3, :])\n",
    "    clr_out = []\n",
    "    for k in range(predictions.shape[0]):\n",
    "        clr_out.append(clrs[predictions[k]])\n",
    "\n",
    "    time = np.arange(predictions.shape[0])/25.\n",
    "    plt.scatter(time, \n",
    "             np.ones(predictions.shape[0]),\n",
    "             c=clr_out)\n",
    "    \n",
    "    # \n",
    "    clr_out = []\n",
    "    for k in range(predictions_high_confidence.shape[0]):\n",
    "        clr_out.append(clrs[predictions_high_confidence[k]])\n",
    "        \n",
    "    time_high_conf = idx_high_conf/25.\n",
    "    plt.scatter(time_high_conf, \n",
    "             np.ones(predictions_high_confidence.shape[0])+1,\n",
    "             c=clr_out)\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Time (sec)\", fontsize=20)\n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.yticks([])\n",
    "    plt.suptitle(\"CNN animal detected: \"+names[max_id] + \"(all frames) \"\n",
    "                 + str(round(np.max(y[0])/np.sum(y[0])*100,2))+\"% of total track\"\n",
    "                 \n",
    "                 + \"\\nCNN animal detected (high confidence predictoin only): \"+names[max_id] + \" \"\n",
    "                 + str(round(np.max(y_high_conf[0])/np.sum(y_high_conf[0])*100,2))+\"% of total track\"\n",
    "                 + \"\\n SLEAP tracklet # \" + selected_track \n",
    "                 + \" (# frames in track \" \n",
    "                 +str(predictions.shape[0])+\")\", fontsize=18)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 56/768 [00:00<00:01, 558.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded:  (768, 200, 200, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [00:01<00:00, 721.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data final [# samples, RGB, width, height]:  torch.Size([768, 3, 224, 224])\n",
      "test data size:  torch.Size([768, 3, 224, 224])\n",
      " # batches:  2   shape :  torch.Size([500, 3, 224, 224])\n",
      "predictions:  (768,) [0 0 0 0 2 0 0 0 0 0]\n",
      "output array:  (768, 4)\n",
      "confidence;  [0.6191806  0.9972656  0.91767675 0.83108366 0.55318564 0.99801034\n",
      " 0.9044059  0.97675544 0.998643   0.83513814]\n",
      "[0 0 0 0 2 0 0 0 0 0]\n",
      "[0.6191806  0.9972656  0.91767675 0.83108366 0.55318564 0.99801034\n",
      " 0.9044059  0.97675544 0.998643   0.83513814]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "root_dir = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/'\n",
    "selected_track = '158'\n",
    "fname = root_dir+selected_track\n",
    "\n",
    "predictions, test_data, confidence = load_training_data_master(fname, \n",
    "                                                                 model,\n",
    "                                                                 device)\n",
    "print (predictions[:10])\n",
    "print (confidence[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main animal  female\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########### PLOT RESULTS ###############\n",
    "########################################\n",
    "    \n",
    "plot_bars(predictions, confidence, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "final inputs:  torch.Size([500, 3, 224, 224])\n",
      "tensor([3, 0, 3, 2, 3, 2, 0, 3, 2, 1], device='cuda:0')\n",
      "500\n",
      "final inputs:  torch.Size([500, 3, 224, 224])\n",
      "tensor([2, 3, 1, 2, 0, 0, 3, 3, 2, 1], device='cuda:0')\n",
      "1000\n",
      "final inputs:  torch.Size([500, 3, 224, 224])\n",
      "tensor([2, 0, 1, 1, 3, 3, 1, 0, 2, 1], device='cuda:0')\n",
      "1500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-505d1b9be90c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.transpose(0,1,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0minput_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input type {} is not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2766\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tobytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2768\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2770\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  \n",
    "for k in range(0,2000,500):\n",
    "    print (k)\n",
    "    inputs = all_data[k:k+500]\n",
    "   \n",
    "    input_array = []\n",
    "    for k in range(inputs.shape[0]):\n",
    "        temp=inputs[k]#.transpose(0,1,2)\n",
    "        temp = train_transforms(temp)\n",
    "        input_array.append(temp)\n",
    "        \n",
    "    inputs = torch.stack(input_array).to(device)\n",
    "  \n",
    "    print (\"final inputs: \", inputs.shape)\n",
    "\n",
    "    # PREDICT;\n",
    "    # ZERO INit\n",
    "    #optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    #preds_array.extend(preds)\n",
    "    \n",
    "    #preds_array = preds\n",
    "    print (preds[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([364, 3, 224, 224])\n",
      "torch.Size([364, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print (inputs.shape)\n",
    "inputs_test = torch.clone(inputs)\n",
    "print (inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 224, 224])\n",
      "torch.Size([500, 3, 224, 224])\n",
      "tensor([0, 2, 0, 3, 3, 0, 0, 3, 3, 2])\n",
      "tensor([0, 2, 0, 3, 3, 0, 0, 3, 3, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "############# PREDICT ON TEST DATA ############\n",
    "###############################################\n",
    "for k1,k2 in trainloader:\n",
    "    print (k1.shape)\n",
    "    \n",
    "    k1 = k1.to(device)\n",
    "    print (k1.shape)\n",
    "    # PREDICT;\n",
    "    outputs = model(k1)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    print (k2[:10])\n",
    "    print (preds[:10])\n",
    "    \n",
    "    plt.imshow(k1[0].cpu().detach().numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2580, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/cnn_training_30mins/animals/1.npy')\n",
    "print (data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2580/2580 [00:04<00:00, 574.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2580, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORM DATA AS REQUIRED BY RESNET (?)\n",
    "train_data = []\n",
    "from tqdm import trange\n",
    "for k in trange(data.shape[0]):\n",
    "    temp2 = train_transforms(data[k])\n",
    "    train_data.append(temp2)  #THIS CAN BE DONE FASTER\n",
    "train_data = torch.stack(train_data)\n",
    "print (train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 0, 0, 3, 3, 1, 2, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "outputs = model(train_data[1000:1500].to(device))\n",
    "_, preds = torch.max(outputs, 1)\n",
    "print (preds[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4449, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/cnn_training_30mins/animals/0.npy')\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print (data[0].shape)\n",
    "id_ = 1010\n",
    "ax=plt.subplot(2,2,1)\n",
    "plt.imshow(data[id_,:,:,0])\n",
    "\n",
    "ax=plt.subplot(2,2,2)\n",
    "plt.imshow(data[id_,:,:,1])\n",
    "\n",
    "ax=plt.subplot(2,2,3)\n",
    "plt.imshow(data[id_,:,:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2 0 0 0 0 0 1 1 3 2 0 1 0 0 2 0 2 2 0 2 2 0 0 2 0 0 2 3 2 0 1 0 2 2 0\n",
      " 2 0 0 2 0 3 0 3 0 2 2 3 3 3 3 2 2 2 2 0 0 2 0 2 3 2 2 2 2 2 2 2 1 2 2 3 2\n",
      " 3 2 0 2 2 1 2 1 0 2 0 2 2 0 1 3 1 1 0 2 1 0 3 0 0 2 2 3 2 1 1 3 3 2 1]\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/media/cat/7e3d5af3-7d7b-424d-bdd5-eb995a4a0c62/dan/cohort1/2020-3-9_12_14_22_815059_compressed/cnn_output_10mins/'\n",
    "tracklet='12'\n",
    "data = np.load(os.path.join(root_dir, tracklet,'pred.npy') )       \n",
    "    \n",
    "print (data)\n",
    "y  = np.histogram(data,bins=np.arange(0,4.1,1))\n",
    "plt.bar(y[1][:-1],y[0], 0.9)\n",
    "plt.xlim(-0.5,4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/home/cat/Downloads/test/999/frame_0030433_id_999.npz')['frame']\n",
    "print (data.shape)\n",
    "plt.imshow(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 224, 224])\n",
      "tensor([0, 0, 1, 2, 3, 0, 2, 2, 0, 2])\n",
      "tensor([0, 0, 1, 2, 3, 0, 2, 1, 0, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lens = []\n",
    "for dir_ in dirs:\n",
    "    fnames = os.listdir(os.path.join(root_dir,dir_))\n",
    "    lens.append(len(fnames))\n",
    "frame_rate = 25\n",
    "lens=np.array(lens)/frame_rate\n",
    "y=np.histogram(lens, bins=np.arange(0,750/frame_rate,1))\n",
    "plt.plot(y[1][:-1], y[0])\n",
    "plt.semilogy()\n",
    "plt.ylabel(\"# tracklets\",fontsize=20)\n",
    "plt.xlabel(\"duration of tracklet (sec)\",fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlim(left=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8373260d0945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtest_img_rotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3.14159\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img_rotated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-8373260d0945>\u001b[0m in \u001b[0;36mrotate_image\u001b[0;34m(image, angle)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrot_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetRotationMatrix2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpAffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 386
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROTATE IMAGE\n",
    "def rotate_image(image, angle):\n",
    "    image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "    result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    return result\n",
    "\n",
    "\n",
    "# FIND CENTRES\n",
    "locs = np.array([[100,145], [125,155], [115,145], [130,160]])\n",
    "\n",
    "# FIND ROTATION USING PCA\n",
    "x = locs[:,0]\n",
    "y = locs[:,1]\n",
    "x = x - np.mean(x)\n",
    "y = y - np.mean(y)\n",
    "coords = np.vstack([x, y])\n",
    "cov = np.cov(coords)\n",
    "evals, evecs = np.linalg.eig(cov)\n",
    "\n",
    "sort_indices = np.argsort(evals)[::-1]\n",
    "x_v1, y_v1 = evecs[:, sort_indices[0]]  # Eigenvector with largest eigenvalue\n",
    "x_v2, y_v2 = evecs[:, sort_indices[1]]\n",
    "theta = np.arctan((x_v1)/(y_v1))  \n",
    "\n",
    "# CHECK THAT HEAD ALWAYS UP\n",
    "test_img = np.zeros((200,200,3),'float32')+np.nan\n",
    "for k in range(locs.shape[0]):\n",
    "    test_img[int(locs[k,0])-3:int(locs[k,0])+3,\n",
    "             int(locs[k,1])-3:int(locs[k,1])+3] = k+1\n",
    "\n",
    "ax=plt.subplot(2,1,1)\n",
    "plt.imshow(test_img)\n",
    "\n",
    "ax=plt.subplot(2,1,2)\n",
    "test_img_rotated = rotate_image(test_img, -theta*180/3.14159)\n",
    "plt.imshow(test_img_rotated)\n",
    "\n",
    "# check if lower values are above higher values or vice versa\n",
    "# convert to 1D array\n",
    "test_img_1D = np.nansum(test_img_rotated,0)\n",
    "\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
