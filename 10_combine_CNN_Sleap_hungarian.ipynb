{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "from scipy import signal\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from tqdm import trange\n",
    "\n",
    "#import glob2\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "#import hdf5storage\n",
    "import csv\n",
    "from tqdm import trange\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "def get_animal_ids(frame,\n",
    "                  chain_ids,\n",
    "                  root_dir,\n",
    "                  vals,\n",
    "                  classes,\n",
    "                  comments=False):\n",
    "    #frame = 988\n",
    "    #print (\"FRAME: \", frame)\n",
    "    #print (\"classes: \", classes.shape)\n",
    "    \n",
    "    dirs = chain_ids[frame]\n",
    "\n",
    "    if comments:\n",
    "        print (\"DIRS: \", dirs)\n",
    "    \n",
    "    animal_ids = []\n",
    "    animal_ids2 = []\n",
    "    threshold_purity1 = 0.5\n",
    "    threshold_purity2 = 0.7\n",
    "\n",
    "    # HUNGARIAN ALGORITHM BASED SEARCH FOR MOST CORRECT IDS\n",
    "    best_ids_cts = np.zeros((5,5),'float32')\n",
    "    animals3 = []\n",
    "    for ctr_dir, dir_ in enumerate(dirs):\n",
    "        #print (\"2nd Loop: \", dir_)\n",
    "        if np.isnan(dir_)==False:\n",
    "            # find the classifier values for that segment\n",
    "            idx = np.where(vals==dir_)[0]\n",
    "\n",
    "            # find predicted id\n",
    "            fnames = np.sort(glob.glob(root_dir+str(int(dir_))+'/*.npy',recursive = False))\n",
    "\n",
    "            ctr_chain = 0\n",
    "            for fname in fnames:\n",
    "                if ('frame_'+str(frame).zfill(7)) in fname:\n",
    "                     break \n",
    "                ctr_chain+=1\n",
    "        \n",
    "            animal_id3 = classes[idx][ctr_chain]\n",
    "            animals3.append(animal_id3)\n",
    "            \n",
    "            idx5 = np.where(classes[idx]<1E10)[0]\n",
    "            classes_local = classes[idx[idx5]]\n",
    "            \n",
    "            classes_unique = np.unique(classes_local, return_counts=True)\n",
    "            #print (\"best id: \", best_id)\n",
    "            \n",
    "            best_ids_cts[ctr_dir, classes_unique[0]]=classes_unique[1]/idx.shape[0]\n",
    "        else:\n",
    "            animals3.append(None)\n",
    "    \n",
    "    # FIRST LOOP TO CHECK ANY GREAT MATCHES AND ZERO OUT EVERYTHING ELSE\n",
    "    for k1 in range(5):\n",
    "        temp = best_ids_cts[k1]\n",
    "        max_val = np.max(temp)\n",
    "        if max_val>threshold_purity2:\n",
    "            idx8 = np.argmax(temp)\n",
    "            best_ids_cts[:,idx8]=0\n",
    "            best_ids_cts[k1,idx8]=max_val\n",
    "    if comments:\n",
    "        print (best_ids_cts)\n",
    "    # RENORMALIZE PROBABILITIES AFTER ZEROING OUT\n",
    "    for k1 in range(5):\n",
    "        sum_ = best_ids_cts[k1].sum()\n",
    "        if sum_>0:\n",
    "            best_ids_cts[k1]*= 1./sum_\n",
    "    if comments:\n",
    "        print (best_ids_cts)    \n",
    "        \n",
    "    # FINAL LOOP TO GRAB WHATEVER IS LEFT\n",
    "    for k1 in range(5):\n",
    "        max_val = np.max(best_ids_cts[k1])\n",
    "        if max_val==0: # or max_val<threshold_purity1:\n",
    "            animal_ids.append(None)\n",
    "            animal_ids2.append(animals3[k1])\n",
    "            continue\n",
    "        else:\n",
    "            idx9 = np.argmax(best_ids_cts[k1])\n",
    "            #print (\"idx9: \", idx9, classes_unique[0])\n",
    "            animal_ids.append(idx9)\n",
    "            animal_ids2.append(animals3[k1])\n",
    "            \n",
    "            best_ids_cts[:,idx9]= 0\n",
    "            # must zero out all the others\n",
    "            \n",
    "    if comments:\n",
    "        print (best_ids_cts)           \n",
    "    return animal_ids, animal_ids2, dirs\n",
    "\n",
    "def get_animal_ids_function(fname_classification,\n",
    "                   fname_traces_inferences,\n",
    "                   chain_ids,\n",
    "                   root_dir,\n",
    "                   comments,\n",
    "                   n_networks):\n",
    "    \n",
    "    fname_out = os.path.split(fname_classification)[0]+\"/animal_arrays.npy\"\n",
    "    if os.path.exists(fname_out)==False:\n",
    "\n",
    "        data = np.load(fname_classification, allow_pickle=True)\n",
    "        vals = data['vals']\n",
    "        frames = np.hstack(data['frames_array'])\n",
    "        print (\"# of lableed frames: \", vals.shape)\n",
    "        classes = data['classes']\n",
    "\n",
    "        reassembled = np.load(fname_traces_inferences)\n",
    "\n",
    "        #reassembled = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_57_12_418305_compressed/pickle/2020-3-16_12_57_12_418305_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences.npz')\n",
    "        tracesx_re = reassembled['tracesx'].T\n",
    "        tracesy_re = reassembled['tracesy'].T\n",
    "        print (\"Lenght of tracessx: \", tracesx_re.shape)\n",
    "\n",
    "        start = 1\n",
    "        end = tracesx_re.shape[0]\n",
    "\n",
    "        # INITIALIZE ARRAYS TO BE SAVED\n",
    "        animal_ids_hungarian = np.zeros((tracesx_re.shape[0], n_networks), 'float32')\n",
    "        animal_arrays = np.zeros((n_networks, tracesx_re.shape[0], 2), 'float32')\n",
    "        for n in trange(start,end, 1):\n",
    "            #ret, frame = original_vid.read()\n",
    "\n",
    "            # animal_ids is the hungarian output;\n",
    "            # animal_ids is the CNN output\n",
    "            animal_ids, animal_ids2, dirs = get_animal_ids(n,\n",
    "                                                        chain_ids,\n",
    "                                                        root_dir,\n",
    "                                                        vals,\n",
    "                                                        classes,\n",
    "                                                        comments)\n",
    "\n",
    "            #print (\"animal_ids: \", animal_ids)\n",
    "            for k in range(0, 14*n_networks,14):\n",
    "                y = tracesx_re[n,k:k+14]\n",
    "                x = tracesy_re[n,k:k+14]\n",
    "\n",
    "                idx = np.where(x!=0)[0]\n",
    "                x = x[idx]\n",
    "                y = y[idx]\n",
    "                if x.shape[0]==0:\n",
    "                    x_ave = None\n",
    "                    y_ave = None\n",
    "                else:\n",
    "                    x_ave = np.nanmean(x)\n",
    "                    y_ave = np.nanmean(y)\n",
    "\n",
    "                if animal_ids[k//14] is None:\n",
    "                    continue\n",
    "                animal_arrays[animal_ids[k//14], n] = x_ave, y_ave\n",
    "\n",
    "            animal_ids_hungarian[n]=animal_ids \n",
    "\n",
    "        np.save(fname_out, animal_arrays)\n",
    "        np.save(fname_out[:-4]+\"_animal_ids_hungarian.npy\", animal_ids_hungarian)\n",
    "    else:\n",
    "        animal_arrays = np.load(fname_out)\n",
    "        animal_ids_hungarian = np.load(fname_out[:-4]+\"_animal_ids_hungarian.npy\")\n",
    "\n",
    "    print (\"DONE making animal_arrays: \", animal_arrays.shape)\n",
    "\n",
    "    return animal_arrays, animal_ids_hungarian\n",
    "\n",
    "def get_single_feature_traces(fname_classification,\n",
    "                              fname_traces_inferences,\n",
    "                              animal_ids_hungarian,\n",
    "                              n_networks,\n",
    "                              features):\n",
    "    \n",
    "    fname_out = os.path.split(fname_classification)[0] + \"/locs_array.npy\"\n",
    "    if os.path.exists(fname_out)==False:\n",
    "    \n",
    "        reassembled = np.load(fname_traces_inferences)\n",
    "\n",
    "        #reassembled = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_57_12_418305_compressed/pickle/2020-3-16_12_57_12_418305_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences.npz')\n",
    "        tracesx_re = reassembled['tracesx'].T\n",
    "        tracesy_re = reassembled['tracesy'].T\n",
    "\n",
    "        locs_array = np.zeros((n_networks, animal_ids_hungarian.shape[0],2),'float32')+np.nan\n",
    "        \n",
    "        for p in ids:\n",
    "            # Plot spine1 location for each animal\n",
    "            #locs = np.zeros(animal_ids_hungarian.shape[0],2,'float32')\n",
    "            for k in range(1, animal_ids_hungarian.shape[0]):\n",
    "                idxb = np.where(animal_ids_hungarian[k]==p)[0]\n",
    "                if idxb.shape[0]>0:\n",
    "                    tempx = tracesx_re[k,features+idxb*14]\n",
    "                    #print (\"tempx: \", tempx.shape)\n",
    "                    tempy = tracesy_re[k, features+idxb*14]\n",
    "                    locs_array[p,k]= tempx, tempy\n",
    "\n",
    "        np.save(fname_out, locs_array)\n",
    "    \n",
    "    else:\n",
    "        locs_array = np.load(fname_out)\n",
    "        \n",
    "    return locs_array\n",
    "\n",
    "\n",
    "def cleanup_traces(locs_array, \n",
    "                  animal_id,\n",
    "                  velocity_max,\n",
    "                  velocity_min,\n",
    "                  velocity_rel_max,\n",
    "                  min_seg_len):\n",
    "    \n",
    "    data = locs_array[animal_id].copy()\n",
    "    \n",
    "    # REMOVE GARBAGE / FAILED PREDICTION\n",
    "    idx = np.where(data[:,0]==0.)[0]\n",
    "    #print (\" ZERO REMOVAL: \", idx.shape)\n",
    "    data[idx,0]=np.nan\n",
    "    data[idx,1]=np.nan\n",
    "\n",
    "    # DEL SHORT SEGS \n",
    "    idx = np.where(np.isnan(data[:,0])==True)[0]\n",
    "    diffs = idx[1:]-idx[:-1]\n",
    "    idx2 = np.where(np.logical_and(diffs>1, diffs<min_seg_len))[0]\n",
    "    #print (\"short segs deletion: \", idx2.shape)\n",
    "\n",
    "    # loop over short segs and remove them\n",
    "    for id_ in idx2:\n",
    "        data[idx[id_]:idx[id_+1]]=np.nan\n",
    "\n",
    "    # VELOCITIES\n",
    "    if True:\n",
    "\n",
    "        if False:\n",
    "            plt.scatter(np.arange(data.shape[0])/25, \n",
    "                    data.sum(1), color='black')\n",
    "            \n",
    "        #temp = temp1[1:]-temp1[:-1]\n",
    "\n",
    "        locs_temp = np.sqrt(data[:,0]**2+data[:,1]**2)\n",
    "        velocity = locs_temp[1:]-locs_temp[:-1]\n",
    "\n",
    "        # absolute velocity delete\n",
    "        idx = np.where(np.abs(velocity)>velocity_max)[0]\n",
    "        data[idx]=np.nan\n",
    "\n",
    "        # relative velocity delete\n",
    "        idx = np.where(np.logical_and(np.abs(velocity[1:])>np.abs(velocity[:-1])*velocity_rel_max,\n",
    "                                     np.abs(velocity[1:])>velocity_min))[0]\n",
    "\n",
    "        #print (\"relative vellcity deletions \", idx.shape)\n",
    "        data[idx]=np.nan   \n",
    "        \n",
    "        idx = np.where(np.logical_and(np.abs(velocity[1:])*velocity_rel_max<np.abs(velocity[:-1]),\n",
    "                                     np.abs(velocity[1:])>velocity_min))[0]\n",
    "\n",
    "        #print (\"relative vellcity deletions \", idx.shape)\n",
    "        data[idx]=np.nan   \n",
    "\n",
    "        \n",
    "\n",
    "    # DEL SHORT SEGS \n",
    "    idx = np.where(np.isnan(data[:,0])==True)[0]\n",
    "    diffs = idx[1:]-idx[:-1]\n",
    "    idx2 = np.where(np.logical_and(diffs>1, diffs<min_seg_len))[0]\n",
    "    #print (\"short segs deletion: \", idx2.shape)\n",
    "\n",
    "    # loop over short segs and remove them\n",
    "    for id_ in idx2:\n",
    "        data[idx[id_]:idx[id_+1]]=np.nan\n",
    "        \n",
    "    locs_array[animal_id] = data\n",
    "\n",
    "    return locs_array\n",
    "\n",
    "def connect_CSUA(locs_array,\n",
    "                       animal_id,\n",
    "                       max_jump, \n",
    "                       max_jump_intra_animal,     \n",
    "                       min_nearby_animal_dist):\n",
    "    \n",
    "    animal_ids = np.arange(4)\n",
    "    \n",
    "    data = locs_array[animal_id].copy()\n",
    "    #print (\"Data: \", data.shape)\n",
    "    \n",
    "    \n",
    "    idx = np.where(np.isnan(data[:,0])==False)[0]\n",
    "   # print (\"interpo: \", idx.shape)\n",
    "   # print (idx[:10])\n",
    "    diffs= idx[1:]-idx[:-1]\n",
    "    idx2 = np.where(diffs>1)[0]\n",
    "    #print (idx[idx2[:10]])\n",
    "    \n",
    "    idxnan = np.where(np.isnan(data[:,0])==True)[0]\n",
    "    diffsnan = idxnan[1:]-idxnan[:-1]\n",
    "    idx2nan = np.where(diffsnan>1)[0]\n",
    "    \n",
    "    for k in range(1,idx2.shape[0]-1,1):\n",
    "        start = idx[idx2[k]]\n",
    "        duration = diffs[idx2[k]]\n",
    "        end = idxnan[idx2nan[k+1]]+1\n",
    "        \n",
    "        dist = np.sqrt((data[start,0]-data[end,0])**2+\n",
    "                       (data[start,1]-data[end,1])**2)\n",
    "        \n",
    "        # CHECK MIN DIST; SET THIS TO CONSERVATIVE VALUE\n",
    "        if dist <= max_jump:\n",
    "                \n",
    "            #print (data[start,0], data[end,0], duration)\n",
    "            data[start:end,0]= np.linspace(data[start,0], data[end,0], duration)\n",
    "            data[start:end,1]= np.linspace(data[start,1], data[end,1], duration)\n",
    "            \n",
    "        # CHECK MORE BROADLY ENSURE NO OTHER ANIMAL NEARBY\n",
    "        elif dist <= max_jump_intra_animal:\n",
    "            for a in animal_ids:\n",
    "                if a == animal_id:\n",
    "                    continue\n",
    "                    #dists_start.append(1E10)\n",
    "                    #dists_end.append(1E10)\n",
    "                    \n",
    "                near_anim = np.sqrt((locs_array[a][start,0]-locs_array[animal_id][start,0])**2+\n",
    "                                      (locs_array[a][start,1]-locs_array[animal_id][start,1])**2)\n",
    "                \n",
    "                if np.min(near_anim)<min_nearby_animal_dist:\n",
    "                    continue\n",
    "                    \n",
    "                near_anim = np.sqrt((locs_array[a][end,0]-locs_array[animal_id][end,0])**2+\n",
    "                                      (locs_array[a][end,1]-locs_array[animal_id][end,1])**2)\n",
    "                \n",
    "                if np.min(near_anim)<min_nearby_animal_dist:\n",
    "                    continue\n",
    "\n",
    "            #print (data[start,0], data[end,0], duration)\n",
    "            data[start:end,0]= np.linspace(data[start,0], data[end,0], duration)\n",
    "            data[start:end,1]= np.linspace(data[start,1], data[end,1], duration)\n",
    "            \n",
    "    locs_array[animal_id] = data\n",
    "    \n",
    "    return locs_array\n",
    "\n",
    "\n",
    "\n",
    "def swap_traces(locs_array, \n",
    "                animal_id,\n",
    "                animal_ids):\n",
    "    '''  Function that tries to correct obvious swaps\n",
    "    '''\n",
    "    \n",
    "    # GRAB CSUAs FOR ANIMAL AND SEE IF BETTER MATCH WITH OTHER STARTS/ENDS\n",
    "    # GRAB CSUAs from main animal\n",
    "    idx = np.where(np.isnan(locs_array[animal_id])==False)[0]\n",
    "    diffs = idx[1:]-idx[:-1]\n",
    "    idx_end = np.where(diffs>1)[0]\n",
    "    \n",
    "    idxnan = np.where(np.isnan(locs_array[animal_id])==True)[0]\n",
    "    diffs = idxnan[1:]-idxnan[:-1]\n",
    "    idx_start = np.where(diffs>1)[0]\n",
    "    \n",
    "    CSUA = np.array([idxnan[idx_start],\n",
    "                    idx[idx_end]]).T\n",
    "    print (\"animal id: \", animal_id, \n",
    "           \"CSUA start: \", CSUA)\n",
    "    \n",
    "    # FIND WHICH CSUAS HAVE LARGE JUMPS FROM NEIGHBOURS\n",
    "    \n",
    "    \n",
    "#     # FIND CSUAs for other animals\n",
    "#     for a in animal_ids:\n",
    "#         idx = np.where(np.isnan(locs_array[a])==False)[0]\n",
    "#         diffs = idx[1:]-idx[:-1]\n",
    "#         idx2 = np.where(diffs>1)[0]    \n",
    "    \n",
    "    \n",
    "    return locs_array\n",
    "\n",
    "\n",
    "\n",
    "def cleanup(fname_classification, \n",
    "            animal_arrays,\n",
    "            animal_ids,\n",
    "            locs_array,\n",
    "            velocity_max = 200,\n",
    "            velocity_min = 10.,\n",
    "            velocity_rel_max = 7.,\n",
    "            min_seg_len = 5,\n",
    "            max_jump = 100, # SAFE DISTANCE TO MERGE SPLIT \n",
    "            max_jump_intra_animal = 500, # DISTANCE TO MERGE IF NO OTHER ANIMALS \"NEARBY\"\n",
    "            min_nearby_animal_dist = 30, # DISTANCE DEFINING \"NEARBY\"\n",
    "            separation_dist = 100, # Distance under which labels can no longer change location\n",
    "           ):\n",
    "\n",
    "    for animal_id in animal_ids:\n",
    "        #print (locs_array[animal_id].shape)\n",
    "\n",
    "        ###################################################\n",
    "        ################## CLEANUP TRACES #################\n",
    "        ###################################################\n",
    "        if True:\n",
    "            locs_array = cleanup_traces(locs_array,\n",
    "                              animal_id,\n",
    "                              velocity_min,\n",
    "                              velocity_min,\n",
    "                              velocity_rel_max,\n",
    "                              min_seg_len)\n",
    "\n",
    "        ###################################################\n",
    "        ############# SWAP TRACES - NOT IMPLEMENTED #######\n",
    "        ###################################################\n",
    "    #     locs_array = swap_traces(locs_array,\n",
    "    #                              animal_id,\n",
    "    #                              animal_ids)\n",
    "\n",
    "        ###################################################\n",
    "        ############## CONNECTS CSUAS  ####################\n",
    "        ###################################################\n",
    "        if True:\n",
    "            # REPEAT INTERPOLATION X TIMES\n",
    "            for k in range(5):\n",
    "                locs_array = connect_CSUA(locs_array,\n",
    "                                                animal_id,\n",
    "                                                max_jump,\n",
    "                                                max_jump_intra_animal,     \n",
    "                                                min_nearby_animal_dist)\n",
    "        ###################################################\n",
    "        ################# INTERPOLATE TRACES ##############\n",
    "        ###################################################\n",
    "        if True:\n",
    "            # Fix x,y positions for missing data to last value in time\n",
    "            # TODO: Try to recover missing data by looking at \n",
    "            #  raw heatmaps or peaks outputed by DLC\n",
    "            locs_array = interpolate_traces(locs_array,\n",
    "                                            animal_id)\n",
    "        \n",
    "        \n",
    "        ###################################################\n",
    "        ######## STOPS TRACES FROM JUMPING/JITTER #########\n",
    "        ###################################################\n",
    "        locs_array = stop_traces_jumps(locs_array, separation_dist)\n",
    "\n",
    "            \n",
    "\n",
    "    return locs_array \n",
    "\n",
    "def interpolate_traces(locs_array,\n",
    "                       animal_id):\n",
    "    \n",
    "    ''' Replace all np.nans by the last good value\n",
    "    '''\n",
    "        \n",
    "    data = locs_array[animal_id].copy()\n",
    "    #print (\"Data: \", data.shape)\n",
    "    \n",
    "    idx = np.where(np.isnan(data)==True)[0]\n",
    "    for id_ in idx:\n",
    "        data[id_]=data[id_-1]\n",
    "    \n",
    "    # go backwoards in time to ensure beginning of time also has non np.nan vals\n",
    "    data = data[::-1]\n",
    "    idx = np.where(np.isnan(data)==True)[0]\n",
    "    for id_ in idx:\n",
    "        data[id_]=data[id_-1]\n",
    "\n",
    "    data = data[::-1]\n",
    "    locs_array[animal_id] = data\n",
    "    \n",
    "    return locs_array\n",
    "\n",
    "def stop_traces_jumps(locs_array, separation_dist):\n",
    "    ''' Function that freezes traces once an animal gets near another\n",
    "    '''\n",
    "    \n",
    "    # COMPUTE INTER-ANIMAL DISTANCES AT EVERY POINT:\n",
    "    \n",
    "    for a in range(4):\n",
    "        \n",
    "        locs_a = locs_array[a]\n",
    "        \n",
    "        for aa in range(4):\n",
    "            if a==aa:\n",
    "                continue\n",
    "            locs_aa = locs_array[aa]\n",
    "            \n",
    "            dists = np.sqrt((locs_a[:,0]-locs_aa[:,0])**2+\n",
    "                            (locs_a[:,1]-locs_aa[:,1])**2)\n",
    "            \n",
    "            idx = np.where(dists<separation_dist)[0]\n",
    "            # search forward in time and fix to last value if too close \n",
    "            for id_ in idx:\n",
    "                locs_a[id_]=locs_a[id_-1]\n",
    "\n",
    "    \n",
    "    return locs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89988, 5)\n",
      "DONE making animal_arrays:  (5, 89988, 2)\n",
      "DONE computing hungarian ID assignments\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "###### COMBINE CNN OUTPUT AND SLEAP OUTPUT + HUNGARIAN ALGORITHM ######\n",
    "#######################################################################\n",
    "\n",
    "# Classifier output for CSUA; using 100 x 100 pixel images;\n",
    "fname_classification = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/classification_output.npz'\n",
    "                        #/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/classification_output.npz\n",
    "\n",
    "# Notebook 11 - CC based image extraction generates chain_id file \n",
    "fname_chain_ids = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences_0_89988_chain_id.npy'\n",
    "chain_ids = np.load(fname_chain_ids)\n",
    "print (chain_ids.shape)\n",
    "\n",
    "# CSUA image locations; required for hungarian; may wish to simplifiy;\n",
    "root_dir = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/training_images/'\n",
    "\n",
    "# Traces post-CC step\n",
    "fname_traces_inferences = '/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences_0_89988.npz'\n",
    "                          #'/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressedDLC_resnet50_madeline_july2Jul2shuffle1_100000_full_traces_inferences_0_89988.npz'\n",
    "\n",
    "comments=False\n",
    "n_networks = 5\n",
    "animal_ids, animal_ids_hungarian = get_animal_ids_function(fname_classification,\n",
    "                                                  fname_traces_inferences,\n",
    "                                                   chain_ids,\n",
    "                                                   root_dir,\n",
    "                                                   comments,\n",
    "                                                   n_networks)\n",
    "print (\"DONE computing hungarian ID assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE with single feature post huganrian (5, 89988, 2)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "####### COMPUTE SINGLE FEATURE TRACES POST HUNGARIAN #########\n",
    "##############################################################\n",
    "features = np.array(6, 'int32') # spine_1\n",
    "ids = np.arange(5)\n",
    "#ids=[0]\n",
    "\n",
    "locs_array = get_single_feature_traces(fname_classification,\n",
    "                                      fname_traces_inferences,\n",
    "                                      animal_ids_hungarian,\n",
    "                                      n_networks,\n",
    "                                      features)\n",
    "\n",
    "print (\"DONE with single feature post huganrian\", locs_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/ipykernel_launcher.py:235: RuntimeWarning: invalid value encountered in greater\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/ipykernel_launcher.py:239: RuntimeWarning: invalid value encountered in greater\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/ipykernel_launcher.py:240: RuntimeWarning: invalid value encountered in greater\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/ipykernel_launcher.py:245: RuntimeWarning: invalid value encountered in less\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/ipykernel_launcher.py:246: RuntimeWarning: invalid value encountered in greater\n",
      "/home/cat/.conda/envs/DLC-GPU/lib/python3.7/site-packages/ipykernel_launcher.py:474: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE Post-processing traces - GO TO Notebook 15_behaviour_analysis\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "###### CLEANUP: Del short segs; connect safe segs ###############\n",
    "#################################################################\n",
    "#clrs = ['red','blue','cyan','green','yellow']\n",
    "clrs = ['blue','red','yellow','green','yellow']\n",
    "\n",
    "    \n",
    "#print (locs_array.shape)\n",
    "animal_arrays = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/animal_arrays.npy')\n",
    "#print (animal_arrays.shape)\n",
    "animal_arrays = locs_array.copy()\n",
    "\n",
    "animal_ids = np.arange(5)\n",
    "locs_array_clean = cleanup(fname_classification, \n",
    "                            animal_arrays,\n",
    "                            animal_ids,\n",
    "                            locs_array)\n",
    "            \n",
    "np.save(os.path.split(fname_classification)[0]+'/locs_array_clean.npy', \n",
    "                       locs_array_clean)\n",
    "\n",
    "print (\"DONE Post-processing traces - GO TO Notebook 15_behaviour_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
